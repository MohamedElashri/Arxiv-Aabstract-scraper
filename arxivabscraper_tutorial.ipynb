{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arxivabscraper-tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPOdjX2denrBzrijyJUJ6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedElashri/Arxiv-Aabstract-scraper/blob/add-license-1/arxivabscraper_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CKmBZ6Ubj5"
      },
      "source": [
        "# Arxiv absrtact scraper tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5WKBAt-Uk1T"
      },
      "source": [
        "by [Mohamed Elashri](http://melashri.net)\r\n",
        "\r\n",
        "*Created: Dec 27th, 2020*\r\n",
        "*Last updated: Dec 27th, 2020*\r\n",
        "\r\n",
        "This is a tutorial about how to use arxivabscraper library to scrap abstracts from the famous research paper archive \"arxiv\". The usage is very simple and this tutorial provides an example of how it works on google colab. \r\n",
        "\r\n",
        "The tutorial is three parts\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1.   Installing arxivabscraper package\r\n",
        "2.   define usage function \r\n",
        "3.   download the the resulted abstracts in a CSV file.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NstvbcSXVhFe"
      },
      "source": [
        "#Installing arxivabscraper package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsGs8Mf31hXv",
        "outputId": "930419f4-727d-4a3e-d5f8-e61b88d57ecd"
      },
      "source": [
        "!pip install arxivabscraper --upgrade\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arxivabscraper\n",
            "  Downloading https://files.pythonhosted.org/packages/01/30/79e7039ae4702938bff798ecdd423d17f38db073e3419573e6a57f73c83c/arxivabscraper-0.3-py3-none-any.whl\n",
            "Installing collected packages: arxivabscraper\n",
            "  Found existing installation: arxivabscraper 0.2\n",
            "    Uninstalling arxivabscraper-0.2:\n",
            "      Successfully uninstalled arxivabscraper-0.2\n",
            "Successfully installed arxivabscraper-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue7fLaXFVpjw"
      },
      "source": [
        "#Usage function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmZrqf_HWMqF"
      },
      "source": [
        "In this example we are scraping all the abstracts from the high energy physics theory category between May 27th 2010 until June 6th 2020. We are passing this as an argument for the function `arxivabscraper.Scraper`. \r\n",
        "\r\n",
        "we specify the category by passing the argument `category='physics:hep-th'`\r\n",
        "\r\n",
        "and to specify the time frame we pass the start date to `date_from='2010-05-27'` and end date to `date_until'2020-06-07'`\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faqtM_nP1nRG"
      },
      "source": [
        "import arxivabscraper\r\n",
        "scraper = arxivabscraper.Scraper(category='physics:hep-th', date_from='2010-05-27',date_until='2020-06-07')\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ot4jpF1xEH",
        "outputId": "9f0b0d00-cd86-4089-b4fa-6263f34ab71f"
      },
      "source": [
        "output = scraper.scrape()\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fetching up to  1000 records...\n",
            "fetching up to  2000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  2000 records...\n",
            "fetching up to  3000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  3000 records...\n",
            "fetching up to  4000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  4000 records...\n",
            "fetching up to  5000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  5000 records...\n",
            "fetching up to  6000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  6000 records...\n",
            "fetching up to  7000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  7000 records...\n",
            "fetching up to  8000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  8000 records...\n",
            "fetching up to  9000 records...\n",
            "Got 503. Retrying after 30 seconds.\n",
            "fetching up to  9000 records...\n",
            "fetching is completed in 334.8 seconds.\n",
            "Total number of records 9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvVYN91MVvyE"
      },
      "source": [
        "# Download results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny5NUy7vVy3O"
      },
      "source": [
        "We can use pandas to convert out scraped text into pandas dataframe and then convert it to a csv file.\r\n",
        "\r\n",
        "We then use `files.download()` function to download the resulting file into local machine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnzoZUI21xfh"
      },
      "source": [
        "import pandas as pd\r\n",
        "cols = ('id','abstract')\r\n",
        "df3 = pd.DataFrame(output,columns=cols)\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pWaNj8UM2POX",
        "outputId": "1d5b86cc-8ca7-47cc-9e41-a524676ecce3"
      },
      "source": [
        "from google.colab import files\r\n",
        "df3.to_csv('df3.csv') \r\n",
        "files.download(\"df3.csv\")\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0ed45ac5-27ac-4c8d-83b1-9d85924fb57a\", \"df3.csv\", 7063109)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}